{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1) (11, 1)\n",
      "tensor(115.5318, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 115.53175354003906\n",
      "tensor(9.4364, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 9.436407089233398\n",
      "tensor(0.7824, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 0.7824001312255859\n",
      "tensor(0.0764, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.0763784721493721\n",
      "tensor(0.0187, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.01865028217434883\n",
      "tensor(0.0138, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.013802875764667988\n",
      "tensor(0.0133, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.013270335271954536\n",
      "tensor(0.0131, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.013091250322759151\n",
      "tensor(0.0129, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.012942560948431492\n",
      "tensor(0.0128, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.012797845527529716\n",
      "tensor(0.0127, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.012654908932745457\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.012513610534369946\n",
      "tensor(0.0124, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.01237383484840393\n",
      "tensor(0.0122, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.012235703878104687\n",
      "tensor(0.0121, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.012099062092602253\n",
      "tensor(0.0120, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.011963924393057823\n",
      "tensor(0.0118, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.011830340139567852\n",
      "tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.011698221787810326\n",
      "tensor(0.0116, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.011567612178623676\n",
      "tensor(0.0114, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.011438436806201935\n",
      "tensor(0.0113, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.011310676112771034\n",
      "tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.01118440181016922\n",
      "tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.011059504002332687\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.010936014354228973\n",
      "tensor(0.0108, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.010813872329890728\n",
      "tensor(0.0107, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.010693120770156384\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.010573712177574635\n",
      "tensor(0.0105, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.01045563817024231\n",
      "tensor(0.0103, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.01033888477832079\n",
      "tensor(0.0102, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.01022341288626194\n",
      "tensor(0.0101, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.010109280236065388\n",
      "tensor(0.0100, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.009996386244893074\n",
      "tensor(0.0099, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.009884734638035297\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.009774361737072468\n",
      "tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.009665208868682384\n",
      "tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.00955726858228445\n",
      "tensor(0.0095, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.009450544603168964\n",
      "tensor(0.0093, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.00934500340372324\n",
      "tensor(0.0092, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.009240662679076195\n",
      "tensor(0.0091, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.00913744792342186\n",
      "tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.009035410359501839\n",
      "tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.008934512734413147\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.008834736421704292\n",
      "tensor(0.0087, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.008736100047826767\n",
      "tensor(0.0086, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.008638535626232624\n",
      "tensor(0.0085, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.008542060852050781\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.008446683175861835\n",
      "tensor(0.0084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.008352366276085377\n",
      "tensor(0.0083, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.008259095251560211\n",
      "tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.008166874758899212\n",
      "tensor(0.0081, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.008075687102973461\n",
      "tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.007985483855009079\n",
      "tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.00789631437510252\n",
      "tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.007808121852576733\n",
      "tensor(0.0077, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.007720944005995989\n",
      "tensor(0.0076, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.007634713314473629\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.007549479138106108\n",
      "tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.007465175818651915\n",
      "tensor(0.0074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.007381792180240154\n",
      "tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.007299377582967281\n",
      "tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.0072178468108177185\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.007137247361242771\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.007057557348161936\n",
      "tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.006978749297559261\n",
      "tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.006900818087160587\n",
      "tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.006823736242949963\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.006747542880475521\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.006672197487205267\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.0065976884216070175\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.00652400404214859\n",
      "tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.00645116250962019\n",
      "tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.00637913029640913\n",
      "tensor(0.0063, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.0063078743405640125\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.006237444933503866\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.006167794112116098\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.006098918151110411\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.006030822638422251\n",
      "tensor(0.0060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.005963487084954977\n",
      "tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.005896886344999075\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.005831039045006037\n",
      "tensor(0.0058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.005765915848314762\n",
      "tensor(0.0057, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.0057015325874090195\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.005637874361127615\n",
      "tensor(0.0056, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.005574923008680344\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.005512664560228586\n",
      "tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.005451099015772343\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.0053902422077953815\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.005330033600330353\n",
      "tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.005270527210086584\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.005211671348661184\n",
      "tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.005153459031134844\n",
      "tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.005095910746604204\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.00503901019692421\n",
      "tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.004982740618288517\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.004927104339003563\n",
      "tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.004872089717537165\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.004817664623260498\n",
      "tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.0047638798132538795\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.004710698034614325\n",
      "tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.0046580820344388485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype = np.float32).reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values ]\n",
    "y_train = np.array(y_values, dtype = np.float32).reshape(-1, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputsize, outputsize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputsize, outputsize)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "inputDim = 1        # takes variable 'x' features of x\n",
    "outputDim = 1       # takes variable 'y' features of y\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
