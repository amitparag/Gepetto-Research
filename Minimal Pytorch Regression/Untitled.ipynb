{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10872.2958984375\n",
      "1000 3366.804443359375\n",
      "2000 524.8036499023438\n",
      "3000 70.91944122314453\n",
      "4000 27.638893127441406\n",
      "5000 10.332592010498047\n",
      "6000 4.092840671539307\n",
      "7000 1.5842965841293335\n",
      "8000 0.7119300365447998\n",
      "9000 0.4461432695388794\n",
      "10000 0.29155704379081726\n",
      "11000 0.1746618151664734\n",
      "12000 0.09589801728725433\n",
      "13000 0.05314129218459129\n",
      "14000 0.03150360658764839\n",
      "15000 0.020917922258377075\n",
      "16000 0.015380112454295158\n",
      "17000 0.012230257503688335\n",
      "18000 0.010033935308456421\n",
      "19000 0.008401062339544296\n",
      "Training lasted = 32 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nplt.figure(1)\\ny_last = model(x)\\nplt.plot(y.detach().numpy(), y_last.detach().numpy(), 'o')\\nplt.xlabel('Targeted y', fontsize=16)\\nplt.ylabel('Modeled y', fontsize=16)\\nplt.savefig('Model_validation')\\n\\nif NINPUT == 2:\\n    xrange = np.linspace(-1,1,100)\\n    xtest = np.array([ [x1,x2] for x1 in xrange for x2 in xrange ])\\n    ytest = datagen(xtest)\\n    ypred = model(torch.Tensor(xtest)).detach().numpy()\\n    plt.figure(2,figsize=[10,15])\\n\\n    trange = [ min(ytest),max(ytest) ]\\n    prange = [ min(ypred),max(ypred) ]\\n    vrange = [ min(trange+prange),max(trange+prange)]\\n    vd  = vrange[1]-vrange[0]\\n    vrange = [ vrange[0]-vd*.1, vrange[1]+vd*.1 ]\\n    \\n    plt.subplot(3,1,1)\\n    plt.scatter(xtest[:,0],xtest[:,1],c=ytest.flat,vmin=vrange[0],vmax=vrange[1])\\n    plt.colorbar()\\n    plt.title('Data')\\n\\n    plt.subplot(3,1,2)\\n    plt.scatter(xtest[:,0],xtest[:,1],c=ypred.flat,vmin=vrange[0],vmax=vrange[1])\\n    plt.colorbar()\\n    plt.title('Prediction')\\n\\n    plt.subplot(3,1,3)\\n    plt.scatter(xtest[:,0],xtest[:,1],c=(ypred-ytest).flat)\\n    plt.colorbar()\\n    plt.title('Error')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "plt.ion()\n",
    "\n",
    "class DataFunction:\n",
    "    def __init__(self,dim=2):\n",
    "        self.dim = dim\n",
    "    @property\n",
    "    def ninput(self): return self.dim\n",
    "    @property\n",
    "    def noutput(self): return 1\n",
    "    def eval(self,x):\n",
    "        # Return the sum of square while taking care of the dimension\n",
    "        return np.array([np.sum(x**2,axis=x.ndim-1)]).T\n",
    "    def __call__(self,x): return self.eval(x)\n",
    "    \n",
    "datagen = DataFunction()\n",
    "\n",
    "NSAMPLE = 1000\n",
    "NINPUT = datagen.ninput\n",
    "NOUTPUT = datagen.noutput\n",
    "NHIDDEN = 64\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = np.random.rand(NSAMPLE, NINPUT)*4-2\n",
    "y = datagen(x)\n",
    "\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(D_in, H)\n",
    "        self.l2 = torch.nn.Linear(H, H)\n",
    "        self.l3 =torch.nn.Linear(H, D_out)\n",
    "\n",
    "        self.activ1 = torch.tanh#torch.nn.ReLU()\n",
    "        self.activ2 = torch.tanh#torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.l3(self.activ2(self.l2(self.activ1(self.l1(X)))))\n",
    "\n",
    "########### Training\n",
    "\n",
    "\n",
    "model = Model(NINPUT,NHIDDEN,NOUTPUT)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "t0 = time.time()\n",
    "for t in range(NSAMPLE*20):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if not t % 1000: print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "print('Training lasted = %.0f seconds' % (time.time()-t0))\n",
    "\n",
    "############  Test/validation\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(1)\n",
    "y_last = model(x)\n",
    "plt.plot(y.detach().numpy(), y_last.detach().numpy(), 'o')\n",
    "plt.xlabel('Targeted y', fontsize=16)\n",
    "plt.ylabel('Modeled y', fontsize=16)\n",
    "plt.savefig('Model_validation')\n",
    "\n",
    "if NINPUT == 2:\n",
    "    xrange = np.linspace(-1,1,100)\n",
    "    xtest = np.array([ [x1,x2] for x1 in xrange for x2 in xrange ])\n",
    "    ytest = datagen(xtest)\n",
    "    ypred = model(torch.Tensor(xtest)).detach().numpy()\n",
    "    plt.figure(2,figsize=[10,15])\n",
    "\n",
    "    trange = [ min(ytest),max(ytest) ]\n",
    "    prange = [ min(ypred),max(ypred) ]\n",
    "    vrange = [ min(trange+prange),max(trange+prange)]\n",
    "    vd  = vrange[1]-vrange[0]\n",
    "    vrange = [ vrange[0]-vd*.1, vrange[1]+vd*.1 ]\n",
    "    \n",
    "    plt.subplot(3,1,1)\n",
    "    plt.scatter(xtest[:,0],xtest[:,1],c=ytest.flat,vmin=vrange[0],vmax=vrange[1])\n",
    "    plt.colorbar()\n",
    "    plt.title('Data')\n",
    "\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.scatter(xtest[:,0],xtest[:,1],c=ypred.flat,vmin=vrange[0],vmax=vrange[1])\n",
    "    plt.colorbar()\n",
    "    plt.title('Prediction')\n",
    "\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.scatter(xtest[:,0],xtest[:,1],c=(ypred-ytest).flat)\n",
    "    plt.colorbar()\n",
    "    plt.title('Error')\n",
    "\"\"\"    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(NSAMPLE, NINPUT)*4-2\n",
    "y = datagen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSAMPLE * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
