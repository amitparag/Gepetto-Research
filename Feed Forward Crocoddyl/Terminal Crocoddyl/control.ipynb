{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import crocoddyl\n",
    "from utils import m2a\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import numdifftools as nd\n",
    "from jacobian import JacobianReg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [12:12<00:00, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lasted = 732 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def state_data(size:int = 1000, theta:float = 0.):\n",
    "    \"\"\"\n",
    "    @params:\n",
    "        1: size  = size of the dataset\n",
    "        2: theta = float, between 0, 1\n",
    "    \n",
    "    Returns xtrain, ytrain.\n",
    "    Returns data in the form of x --> V(x)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _xtrain = []\n",
    "    _ytrain = []\n",
    "\n",
    "    \n",
    "    for _ in range(size):\n",
    "        # Generate random starting configuration\n",
    "        xyz = [np.random.uniform(-2.1, 2.1), \n",
    "               np.random.uniform(-2.1, 2.1),\n",
    "               theta]\n",
    "        \n",
    "        \n",
    "        model = crocoddyl.ActionModelUnicycle()\n",
    "        T = 30\n",
    "        model.costWeights = np.matrix([1,1]).T\n",
    "        \n",
    "        problem = crocoddyl.ShootingProblem(m2a(xyz).T, [ model ] * T, model)\n",
    "        ddp = crocoddyl.SolverDDP(problem)\n",
    "        ddp.solve([], [], 1000)\n",
    "        xs = m2a(ddp.us).flatten()\n",
    "        _ytrain.append(xs)        \n",
    "        _xtrain.append(xyz)\n",
    "        \n",
    "    xtrain = torch.tensor(_xtrain, dtype = torch.float32)\n",
    "    ytrain = torch.tensor(_ytrain, dtype = torch.float32)\n",
    "        \n",
    "    return xtrain, ytrain\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_features,\n",
    "                 output_features,\n",
    "                 n_hidden_units = 512,\n",
    "                 activation = 'relu'):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        A two hidden layered neural network.\n",
    "        \n",
    "        @params:\n",
    "            1: input_features = number of input features of the dataset\n",
    "            2: output_features = number of output features of the dataset\n",
    "            3: n_hidden_units = number of hidden units in hidden layer\n",
    "            4: activation = either relu or tanh\n",
    "            \n",
    "            \n",
    "        @returns:\n",
    "            A fully connected feedforward network \n",
    "        \n",
    "        \"\"\"       \n",
    "        \n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "            \n",
    "        else:\n",
    "            self.activation = torch.tanh\n",
    "        # Structure\n",
    "        self.fc1 = nn.Linear(input_features, n_hidden_units)\n",
    "        self.fc2 = nn.Linear(n_hidden_units, n_hidden_units)\n",
    "        self.fc3 = nn.Linear(n_hidden_units, output_features)\n",
    "        \n",
    "        # Initialization protocol\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "      \n",
    "        self.device = torch.device('cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):     \n",
    "        \n",
    "        x1 = self.activation(self.fc1(x)) \n",
    "        x2 = self.activation(self.fc2(x1)) \n",
    "        x3 = self.fc3(x2) \n",
    "        return x3\n",
    "\n",
    "xtrain, ytrain = state_data(1000)\n",
    "dataset = torch.utils.data.TensorDataset(xtrain,ytrain)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 1000) # DataGenerator\n",
    "\n",
    "\n",
    "# Generate a Neural Net\n",
    "net = FeedForwardNet(input_features = xtrain.shape[1], \n",
    "                     output_features = ytrain.shape[1],\n",
    "                     n_hidden_units = 256)\n",
    "\n",
    "# set the net to training mode\n",
    "net = net.float()\n",
    "net.train()\n",
    "\n",
    "\n",
    "# Define the loss function for optimizer\n",
    "criteria = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay = 0.1)   \n",
    "\n",
    "# Jacobian regularization\n",
    "reg = JacobianReg() \n",
    "lambda_JR = 0.01 \n",
    "\n",
    "t0 = time.time()    \n",
    "# Training    \n",
    "for epoch in tqdm(range(10000)):        \n",
    "    for data, target in dataloader:   \n",
    "        data.requires_grad=True\n",
    "        optimizer.zero_grad()\n",
    "        output = net.forward(data)          \n",
    "        loss_super = criteria(output, target)\n",
    "        R = reg(data, output)                            # Jacobian regularization\n",
    "        loss = loss_super + lambda_JR*R                  # full loss\n",
    "        loss.backward()\n",
    "        optimizer.step()                                      \n",
    "\n",
    "print('Training lasted = %.0f seconds' % (time.time()-t0))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error during testing is 0.0007638324750587344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FeedForwardNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "xtest, ytest = state_data(1000)\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = net(xtest)\n",
    "    \n",
    "mean_squared_error = (ytest - y_pred) **2\n",
    "    \n",
    "print(f\"Mean Squared Error during testing is {torch.mean(mean_squared_error)}\") \n",
    "\n",
    "torch.save(net, \"ControlNet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
