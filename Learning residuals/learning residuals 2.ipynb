{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import crocoddyl as c\n",
    "import numdifftools as nd\n",
    "c.switchToNumpyArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the residual data\n",
    "positions = []\n",
    "residuals = []\n",
    "model = c.ActionModelUnicycle()\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    \n",
    "    x0 = np.array([np.random.uniform(-2.1, 2.1), np.random.uniform(-2.1, 2.1), np.random.uniform(0,1)])\n",
    "    T = 30\n",
    "    problem = c.ShootingProblem(x0.T, [ model ] * T, model)\n",
    "    ddp = c.SolverDDP(problem)\n",
    "    ddp.solve()\n",
    "\n",
    "    for d in ddp.datas():\n",
    "        residual = d.r\n",
    "    positions.append(x0)\n",
    "    residuals.append(residual)\n",
    "\n",
    "positions = np.asarray(positions)\n",
    "residuals = np.asarray(residuals)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 5])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.as_tensor(positions, device = device, dtype = torch.float32)\n",
    "y_train = torch.as_tensor(residuals, device = device, dtype = torch.float32)\n",
    "print(y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=3, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(3, 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.fc3 = nn.Linear(8, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "net = net.float()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model\n",
    "    if classname.find('Linear') != -1:\n",
    "        y = m.in_features\n",
    "    # m.weight.data shoud be taken from a normal distribution\n",
    "        m.weight.data.normal_(0.0,1/np.sqrt(y))\n",
    "    # m.bias.data should be 0\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "net.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Loss :  30.94287591986358\n",
      "Epoch :  1 Loss :  4.403426186647266\n",
      "Epoch :  2 Loss :  3.352968043880537\n",
      "Epoch :  3 Loss :  1.7456619532313198\n",
      "Epoch :  4 Loss :  0.7955069093150087\n",
      "Epoch :  5 Loss :  0.5252071935683489\n",
      "Epoch :  6 Loss :  0.3610587641596794\n",
      "Epoch :  7 Loss :  0.24468998490192462\n",
      "Epoch :  8 Loss :  0.18974918192543555\n",
      "Epoch :  9 Loss :  0.1600945186946774\n",
      "Epoch :  10 Loss :  0.13214549219992477\n",
      "Epoch :  11 Loss :  0.10875453901826404\n",
      "Epoch :  12 Loss :  0.09284755276166834\n",
      "Epoch :  13 Loss :  0.08137349266326055\n",
      "Epoch :  14 Loss :  0.07356259170046542\n",
      "Epoch :  15 Loss :  0.0690423114792793\n",
      "Epoch :  16 Loss :  0.0655575795826735\n",
      "Epoch :  17 Loss :  0.062085748664685525\n",
      "Epoch :  18 Loss :  0.05900180100434227\n",
      "Epoch :  19 Loss :  0.05714824477036018\n",
      "Epoch :  20 Loss :  0.05618440467878827\n",
      "Epoch :  21 Loss :  0.05551967022984172\n",
      "Epoch :  22 Loss :  0.05491925545720733\n",
      "Epoch :  23 Loss :  0.05435348484388669\n",
      "Epoch :  24 Loss :  0.05383762993005803\n",
      "Epoch :  25 Loss :  0.05338262156874407\n",
      "Epoch :  26 Loss :  0.05299696521979058\n",
      "Epoch :  27 Loss :  0.05267733171058353\n",
      "Epoch :  28 Loss :  0.05240834389041993\n",
      "Epoch :  29 Loss :  0.05218940836493857\n",
      "Epoch :  30 Loss :  0.05200699840133893\n",
      "Epoch :  31 Loss :  0.05184686141365091\n",
      "Epoch :  32 Loss :  0.051708369828702416\n",
      "Epoch :  33 Loss :  0.051587360412668204\n",
      "Epoch :  34 Loss :  0.05147855719405925\n",
      "Epoch :  35 Loss :  0.051380666580371326\n",
      "Epoch :  36 Loss :  0.05129204042896163\n",
      "Epoch :  37 Loss :  0.05120852312393254\n",
      "Epoch :  38 Loss :  0.05113506929774303\n",
      "Epoch :  39 Loss :  0.05106622828316176\n",
      "Epoch :  40 Loss :  0.051000741146708606\n",
      "Epoch :  41 Loss :  0.05094148841453716\n",
      "Epoch :  42 Loss :  0.05089194421816501\n",
      "Epoch :  43 Loss :  0.05083962473509018\n",
      "Epoch :  44 Loss :  0.050789654418622376\n",
      "Epoch :  45 Loss :  0.05074931072886102\n",
      "Epoch :  46 Loss :  0.05070366600921261\n",
      "Epoch :  47 Loss :  0.05067139584207325\n",
      "Epoch :  48 Loss :  0.050628703451366164\n",
      "Epoch :  49 Loss :  0.050594932017702376\n",
      "Epoch :  50 Loss :  0.05056170163152274\n",
      "Epoch :  51 Loss :  0.05052959870226914\n",
      "Epoch :  52 Loss :  0.050500261546403635\n",
      "Epoch :  53 Loss :  0.05046924985435908\n",
      "Epoch :  54 Loss :  0.050439276128599886\n",
      "Epoch :  55 Loss :  0.05041718990469235\n",
      "Epoch :  56 Loss :  0.05038449992935057\n",
      "Epoch :  57 Loss :  0.050354161219729576\n",
      "Epoch :  58 Loss :  0.05032788910466479\n",
      "Epoch :  59 Loss :  0.050304796153795905\n",
      "Epoch :  60 Loss :  0.050277563106646994\n",
      "Epoch :  61 Loss :  0.05025066182133742\n",
      "Epoch :  62 Loss :  0.05022535395255545\n",
      "Epoch :  63 Loss :  0.05020035341658513\n",
      "Epoch :  64 Loss :  0.05017441230302211\n",
      "Epoch :  65 Loss :  0.050150404476880794\n",
      "Epoch :  66 Loss :  0.0501224335457664\n",
      "Epoch :  67 Loss :  0.050096869799745036\n",
      "Epoch :  68 Loss :  0.05007351945096161\n",
      "Epoch :  69 Loss :  0.050051014019118156\n",
      "Epoch :  70 Loss :  0.05002109893030138\n",
      "Epoch :  71 Loss :  0.0500029469076253\n",
      "Epoch :  72 Loss :  0.04996955913156853\n",
      "Epoch :  73 Loss :  0.049943665304454044\n",
      "Epoch :  74 Loss :  0.04992491542361677\n",
      "Epoch :  75 Loss :  0.04989374010983738\n",
      "Epoch :  76 Loss :  0.0498716093134135\n",
      "Epoch :  77 Loss :  0.04984545725528733\n",
      "Epoch :  78 Loss :  0.049819625171949156\n",
      "Epoch :  79 Loss :  0.04979637003634707\n",
      "Epoch :  80 Loss :  0.049769755292800255\n",
      "Epoch :  81 Loss :  0.04974392348231049\n",
      "Epoch :  82 Loss :  0.049716698933480075\n",
      "Epoch :  83 Loss :  0.04969698681816226\n",
      "Epoch :  84 Loss :  0.049668723844661145\n",
      "Epoch :  85 Loss :  0.049639298809779575\n",
      "Epoch :  86 Loss :  0.049620029367360985\n",
      "Epoch :  87 Loss :  0.04958934370733914\n",
      "Epoch :  88 Loss :  0.0495686146641674\n",
      "Epoch :  89 Loss :  0.04954045460908674\n",
      "Epoch :  90 Loss :  0.04951825714306324\n",
      "Epoch :  91 Loss :  0.049490255249111215\n",
      "Epoch :  92 Loss :  0.049471456855826546\n",
      "Epoch :  93 Loss :  0.04944211107795127\n",
      "Epoch :  94 Loss :  0.04941766971023753\n",
      "Epoch :  95 Loss :  0.04939778491097968\n",
      "Epoch :  96 Loss :  0.049367927174898796\n",
      "Epoch :  97 Loss :  0.04934687567947549\n",
      "Epoch :  98 Loss :  0.04932314973848406\n",
      "Epoch :  99 Loss :  0.049300147122266935\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, target in zip(x_train, y_train):   \n",
    "        optimizer.zero_grad()\n",
    "        y_hat = net(x_train.float())\n",
    "        loss = criterion(y_hat, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(\"Epoch : \", epoch , \"Loss : \", total_loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2m(a):\n",
    "    return np.matrix(a).T\n",
    "\n",
    "def m2a(m):\n",
    "    return np.array(m).squeeze()\n",
    "c.switchToNumpyMatrix()\n",
    "class UnicycleTerminal(c.ActionModelAbstract):\n",
    "    def __init__(self, net):\n",
    "        c.ActionModelAbstract.__init__(self, c.StateVector(3), 2, 5)\n",
    "        self.net = net\n",
    "        self.dt = .1\n",
    "        self.costWeights = [10., 1.]\n",
    "        \n",
    "    def calc(self, data, x, u=None):\n",
    "        x = torch.as_tensor(x.reshape(1, -1), device = device, dtype = torch.float32)\n",
    "        prediction = self.net(x).view(5, 1)\n",
    "        r_ = prediction.cpu()\n",
    "        data.r = r_.detach().numpy()\n",
    "        #data.r = a2m(prediction.cpu().detach().numpy())\n",
    "        data.cost = 0.5 * float(torch.matmul(prediction.T, prediction))\n",
    "        #print(float(prediction.T @ prediction))\n",
    "            \n",
    "        \n",
    "    def calcDiff(self, data, x, u=None, recalc=True):\n",
    "        if u is None:\n",
    "            u = self.unone\n",
    "        if recalc:\n",
    "            self.calc(data, x, u)\n",
    "\n",
    "        v, w = m2a(u)\n",
    "        px, py, theta = m2a(x)\n",
    "        # Cost derivatives\n",
    "        data.Lx = a2m(m2a(x) * ([self.costWeights[0]**2] * self.state.nx))\n",
    "        data.Lu = a2m(m2a(u) * ([self.costWeights[1]**2] * self.nu))\n",
    "        data.Lxx = np.diag([self.costWeights[0]**2] * self.state.nx)\n",
    "        data.Luu = np.diag([self.costWeights[1]**2] * self.nu)\n",
    "        # Dynamic derivatives\n",
    "        c, s, dt = np.cos(theta), np.sin(theta), self.dt\n",
    "        v, w = m2a(u)\n",
    "        data.Fx = np.matrix([[1, 0, -s * v * dt], [0, 1, c * v * dt], [0, 0, 1]])\n",
    "        data.Fu = np.matrix([[c * self.dt, 0], [s * self.dt, 0], [0, self.dt]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_model = UnicycleTerminal(net)\n",
    "model = c.ActionModelUnicycle()\n",
    "start = perf_counter()\n",
    "for _ in range(1000):\n",
    "    x0 = np.array([np.random.uniform(-2.1, 2.1), np.random.uniform(-2.1, 2.1), np.random.uniform(0,1)])\n",
    "    T = 30\n",
    "    problem = c.ShootingProblem(x0.T, [ model ] * T, terminal_model)\n",
    "    ddp = c.SolverDDP(problem)\n",
    "    ddp.solve([], [], 1000)\n",
    "end = perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859.6277446029999\n"
     ]
    }
   ],
   "source": [
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
